\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}                
\geometry{letterpaper}                   
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{natbib}
\usepackage{amssymb, amsmath}
\usepackage{esvect}
\usepackage{dsfont}
\usepackage{tensor}
\usepackage{slashed}
\usepackage{float}
\usepackage{bbm}
\usepackage{mathtools} % für coloneqq
\usepackage{caption} % für captions und referenzieren von Gleichungen
\usepackage{url}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}


\begin{document}


\input{cover}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\section*{Agreement for free-download}
\bigskip


\bigskip


\large We hereby agree to make our source code for this project freely available for download from the web pages of the SOMS chair. Furthermore, we assure that all source code is written by ourselves and is not violating any copyright restrictions.

\begin{center}

\bigskip


\bigskip


\begin{tabular}{@{}p{3.3cm}@{}p{6cm}@{}@{}p{6cm}@{}}
\begin{minipage}{3cm}

\end{minipage}
&
\begin{minipage}{6cm}
\vspace{2mm} \large Name 1

 \vspace{\baselineskip}

\end{minipage}
&
\begin{minipage}{6cm}

\large Name 2

\end{minipage}
\end{tabular}


\end{center}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



% IMPORTANT
% you MUST include the ETH declaration of originality here; it is available for download on the course website or at http://www.ethz.ch/faculty/exams/plagiarism/index_EN; it can be printed as pdf and should be filled out in handwriting

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\abstract

\setcounter{tocdepth}{2}
\tableofcontents

\newpage





\section{Individual contributions}

\section{Introduction and Motivations}

\section{Description and Proof of the G2-Threshold Theorem}
For any given epidemiological model, there are two key components that will describe our system; namely the virus propagation model and the topological constraints of the system.  It will be shown that these two components can in fact be de-coupled and described by two constants.  To fully describe arbitrary systems, some definitions must be introduced. \\
\underline{Definition:}\\
The connectivity matrix is a symmetric $n \times n$ matrix, where $n$ is the number of nodes in the system.  If two nodes, $i$ and $j$, are connected, the $(i,j)^{th}$ entry is equal to $1$.  Otherwise it is equal to $0$.  In addition, all diagonal entries are automatically set to $0$.\\
Throughout this paper, the connectivity matrix will fully describe the interaction between two different people.  In later chapters, we may allow values ranging from $0$ to $1$, to  better describe the relationship of any two given nodes.  \\
To mathematically characterize any given virus propagation model (VPM), the following parameters are needed. \\
\underline{Parameters:}\\
$(i)$ $\beta$ is the attack or transmission probability via a link between two nodes \\
$(ii)$ $\gamma$ is the immunization loss probability after a nodes has recovered \\
$(iii)$ $\delta$ is the healing probability once a node has been infected \\
$(iv)$ $\epsilon$ is the virus maturation probability after a node has been exposed.  This therefore means that $1-\epsilon$ defines the virus incubation probability \\
$(v)$ $\theta$ is the direct immunization probability whilst a node is susceptible to the virus \\
We now have the tools to prove the theorem, which not only allows us to decouple the topology from the VPM, but also determines whether or not viruses exceed the epidemic threshold. \\
\subsection{G2-Threshold Theorem}
For any given VPM that can be thought of as a $S^* I^2 V^*$ model (there are two infectious states, but not conditions for the number of susceptible and vigilant states), which operates on an arbitrary topology, the virus will be eradicated quickly if 
 \begin{eqnarray}
 s < 1
 \end{eqnarray}
 where s is the effective strength and is defined as 
  \begin{eqnarray}
 s = \lambda_1 \cdot C_{VPM}
 \end{eqnarray}
 Here, $\lambda_1$ is the largest eigenvalue of the connectivity matrix and $C_{VPM}$ is a constant that is derived from the VPM. % Before we prove this theorem, we shall first show how $C_{VPM}$ is attained and explicitly write it down for specific cases.%
 \\
 \subsection{Proof Overview}
 We shall start by first giving a broad overview of why the G2-Threshold Theorem makes sense and in the following subsection we shall go into more detail.  \\
 \underline{Terminology:}\\
 In the general $S^* I^2 V^*$ model, we have the following three classes:\\
$(i)$ Susceptible Class:  Elements of this state can be infected by neighboring elements; i.e. the $i^{th}$ node can be infected by the $j^{th}$ node if $A_{i,j}=1$.  We can have $n \geq 0$ arbitrary susceptible states.  In addition, nodes can only enter this class via endogenous transitions from the to two other classes.  This means that these transitions do not depend on neighboring elements. Transitions within this class are also endogenous.\\
$(ii)$ Infected Class: Elements in one of the two possible infected classes can spread the infection to susceptible neighbors with a certain probability.  For those wondering why we have two classes instead of one, we need only give the following example.  If infectious state I has a transmissibility parameter $\beta_I>0$  and infectious state E with  $\beta_E=0$, we can elegantly describe scenarios where the disease need not always be contagious.  Elements can enter this class via exogenous transitions (neighbor-dependent) from susceptible states.  Transitions within this class are endogenous.  \\
$(iii)$ Vigilant Class:  Elements of these states can neither infect, nor be infected.  This class can be used to describe dead, vaccinated, and/or recovered nodes (only in models, where recovered patients are no longer susceptible).  Once again, there can be $m \geq 0$ possible states in this class.  Elements can enter this class via endogenous transitions from all other states. 
Transitions within this class are also endogenous. \\
\underline{Assumptions:}\\
We will be making two assumptions, which not only simplify calculations, but are rather intuitive.  \\
$(i)$ The infection can only be spread from node $i$ to node $j$ if $A_{i,j}=1$. In essence, this means that the infection can only be spread via neighbors. \\
$(ii)$ When an element from a susceptible state becomes infected, it must always transition into the "first" susceptible state $I_1$.  This is trivially fulfilled for models with one infectious state.  Models with two susceptible states could then describe infections that first need an incubation period.  In this case $I_1$ would be the state "E" that was mentioned before.  \\
\underline{Proof Sketch:}\\
We start by defining a vector $\textbf{P}_t$ that will describe the system at time $t$.  In a model where $m$ is the total number of states and $n$ is the total number of nodes, this vector will have $m \times n$ entries, i.e. $\bold{P}_t \in \mathbb{R}^{m \cdot n} $. More specifically, we can write $\bold{P}_t$ in the following way: \\
\begin{eqnarray}
 \bold{P}_t = (P_{s_1,1,t},P_{s_1,2,t},...,P_{s_1,n,t},P_{s_2,1,t},...,P_{s_m,n,t})^T ,
 \end{eqnarray}
 where $P_{s_i,j,t}$ is given by the probability that node $j$ is an element of state $s_i$ at time $t$.  For any given VPM, $\bold{P}_{t+1}$ can be determined by $\bold{P}_t$, i.e.
 \begin{eqnarray}
 \bold{P}_{t +1}=  g( \bold{P}_t),
 \end{eqnarray}
 where $g$ is a (potentially non-linear) function.  For a fixed point, we have that 
 \begin{eqnarray}
 \bold{P}_{t+1} =  \bold{P}_t.
  \end{eqnarray}
  At these points, one can calculate the stability, i.e. will we return to this fixed point under small perturbations or not, by analyzing the spectrum of the Jacobi matrix at these locations.  We will in essence calculate the stability of systems, where no nodes are infected.  As we will see, the constraints for the spectrum of the Jacobian will allow us to decouple the topology from the VPM and deliver the desired result.
\subsection{Proof}
\label{proof}
To make the proof more readable, we shall introduce additional notation \\
\subsubsection{Notation}
$(i)$ $m$ denotes the total number of states in the VPM. \\
$(ii)$ $q$ denotes the total number of states in both the Susceptible, as well as the Vigilant class.  This means that $m=q+2$. \\
$(iii)$ $w$ denotes the total number of states in the Susceptible Class. \\
$(iv)$ $S_1,...,S_w$ denote the states in the Susceptible class. \\
$(v)$ $E$, $I$ denote the two states in the Infected class. \\
$(vi)$ $\alpha_{KU}$ denotes the probability of a transition from state K to state U.  We assume that $ \alpha_{KU}$ is not only constant, but also given to us beforehand. \\
$(vii)$ $\beta_1$ denotes the transmission probability to for state $E$. \\
$(viii)$ $\beta_2$ denotes the transmission probability to for state $I$. \\
$(ix)$ $\zeta_{i,t} (E,I)$ denotes the probability of node $i$ not being infected by $E$ or $I$ at time $t$. \\
$(x)$ $\bold{x}$ denotes the fixed point vector of our system that has no infected node. \\
$(xi)$  $p_{S_y}^\star$ denotes the probability of being an element of the $S_y$ state at vector $\bold{x}$.  We assume that this value is independent of the node.\\
$(xii)$ $\mathcal{J}$ denotes the Jacobian matrix of the non-linear differential equation at $\bold{x}$.
\subsubsection{Deriving the System Equations}
We will now describe the function $g$ in terms of our parameters.  The first step will be to derive the probability that an arbitrary node $i$ will not be infected by its neighbors during one time-step.  In other words, we want to calculate $\zeta_{i,t} (E,I)$.  We know that the virus will not be transmitted if one of the following holds: \\
$(i)$ The neighbor is not in any of the two infected states E or I. \\
$(ii)$ A neighbor is in state E, but the virus is not transmitted with a probability of $1- \beta_1$ \\
$(iii)$ A neighbor is in state I, but the virus is not transmitted with a probability of $1- \beta_2$ \\
To only consider first-order effects, we shall be assuming that our time steps are infinitesimal, i.e. $\Delta t \rightarrow 0$.  In addition, we postulate that all neighbors are independent of one another. By doing so, we can write $\zeta_{i,t} (E,I)$ as follows:
\begin{eqnarray}
 \zeta_{i,t} (E,I) =  \prod_{J \in N \mathcal{E}(i)} (P_{E,j,t} (1-\beta_1) + P_{I,j,t} (1-\beta_2) + (1- (P_{E,j,t}-(P_{I,i,t})),
 \end{eqnarray}
 where $N \mathcal{E}(i)$ is just the set of all neighboring nodes.  This just says that for each neighbor, we must consider him to be in $(i)$ state $E$ with probability $P_{E,j,t}$ and not spread the disease with probability $(1-\beta_1)$,  $(ii)$ state $I$ with probability $P_{I,j,t}$ and not spread the disease with probability $(1-\beta_2)$, and $(iii)$ none of the Infectious states with probability $(1- (P_{E,j,t}-(P_{I,i,t}))$, where he cannot spread the disease at all.  As all neighbors are independent, we must multiply the probabilities.  We can rewrite equation $(6)$ and attain
 \begin{eqnarray}
 \zeta_{i,t} (E,I) =\prod_{J \in \{ 1,...,n\}} ( 1- \bold{A}_{i,j} ( \beta_1 P_{E,j,t} + \beta_2 P_{I,j,t})).
  \end{eqnarray}
 Here, we just count over all the indices and if $i$ and $j$ are neighbors $A_{i,j}=1$ and we get the same terms as above.  If, on the other hand $i$ and $j$ are not neighbors, $A_{i,j}=0$ and we are in essence multiplying everything by $1$, which changes absolutely nothing. \\
We also demand that for any node $i$ at any given time $t$, it must be in exactly one of the states.  This just means that
\begin{eqnarray}
\forall i,t: \sum_{K} P_{K,i,t} =1.
\end{eqnarray}
Having finally found a formula for $\zeta_{i,t} (E,I)$, we are now ready determine $g$.  If a node is in a particular state $S_y$, which is located in the Susceptible class at time $t+1$, it must be for one of the following reasons: \\
$(i)$ The node was already an element of the state $S_y$ and stayed there. \\
$(ii)$ The node was in a different state $U$ and transitioned into that state during our time step.  This transition must of course be internal (See Terminology). \\
We can now use this information to attain the following result:
\begin{eqnarray}
\forall y=1,2,...,w: P_{S_{y},i,t+1} = \sum_{K \neq S_y} \alpha_{KS_{y}} P_{K,i,t}+ P_{S_{y},i,t} \left( \zeta_{i,t} (E,I) - \sum_{K \neq E, S_{y} ,I} \alpha_{S_y,K}  \right)
\end{eqnarray}
The term $\alpha_{KE} P_{K,i,t}$ denotes all the transitions from other susceptible state to $S_y$, whereas the second term gives us the probability that the element in $S_y$ stays in $S_y$.  Through analogous reasoning we attain similar results for elements in state $E$ at time $t+1$
\begin{eqnarray}
P_{E,i,t+1} = \sum_{K \neq S_1,...S_w} \alpha_{KE} P_{K,i,t}+ \sum_{y= 1}^w P_{S_{y},i,t}  \left( 1 - \zeta_{i,t} (E,I)  \right) ,
\end{eqnarray}
as well as for all other states $U \neq \{ S_1,...,S_w,E \}$ at time t+1
\begin{eqnarray}
P_{U,i,t+1} = \sum_{K} \alpha_{KU}  P_{K,i,t}.
\end{eqnarray}
Equations (9) to (11) now define $g$, which is what we wanted.\\
We will now need a theorem, which will not be proven (e.g. see \cite{Stability}). \\
  \underline{Asymptotic Stability Theorem:}\\
  A system that is determined by $\bold{P}_{t+1} = g(\bold{P}_t)$ is asymptotically stable at some fixed point $\bold{P} = \bold{x}$, if the absolute value of all eigenvalues of the the Jacobian $\mathcal{J} = \nabla g(\bold{x})$ are less than one.  The entries of the Jacobian matrix are given by 
  \begin{eqnarray}
  \mathcal{J}_{i,j} = \left[ \nabla g(\bold{x}) \right]_{i,j} = \frac{\partial g_i}{\partial x_j}|_\bold{x}
  \end{eqnarray}
  \subsubsection{A More Thorough Analysis of Fixed Points}
  We are only interested in fixed points, where none of the nodes are infected.  If that is the case, the only possible transitions will be from endogenous transitions within and between the Susceptible and Vigilant class.  This means that now the topology plays no role whatsoever in determining our preferred fixed point, which allows us to use the Markov Chain model. Using this method, we will attain a unique steady state probability for each node (same for each one). The steady state vector $\pi^\star$, whose entries will contain the probability that any node is found in an arbitrary state (In either the Susceptible or Vigilant class).  It therefore is a vector with $q$ entries, where $q$ is the number of states in the Susceptible and Vigilant class.  We then know from Markov chain analysis that 
  \begin{eqnarray}
  \pi^{\star T} \text{Trans}_{MC_{SV}} =   \pi^{\star} ,
  \end{eqnarray}
  where we set the constraint that 
  \begin{eqnarray}
  \sum_{i=1}^q \pi^{\star} = 1.
  \end{eqnarray}
  Here, $\text{Trans}_{MC_{SV}}$ denotes the stochastic matrix of the Markov chain $MC_{SV}$.  To create a probability vector for an arbitrary node $i$ with has entries for all states, including the Infected states, we just add two $0$ entries to $ \pi^{\star}$.  We  shall call this new vector $p^\star$.  To finally attain the fixed point that describes all nodes, we define $\bold{x}$ to be 
  \begin{eqnarray}
  \bold{x} = \left( p^\star,  p^\star, ..., p^\star \right)^T ,
  \end{eqnarray}
where $ p^\star$ is included $n$ times.  We define $p_{S_{y}}^\star$ to be the steady state probability of an arbitrary node $i$ being an element in state $S_y$.  In addition, let \begin{eqnarray}
p_S^\star = \sum_{y=1}^w p_{S_{y}}^\star .
\end{eqnarray}
This can be interpreted as the probability of any given node $i$ being in the Susceptible class.
 \subsubsection{Deriving the Jacobian}
 Here, we rely on equations $(9)$ to $(11)$ to express $g$ and then take all of the partial derivatives.  As all of our $n$ nodes can be elements of $m$ possible states, we expect our Jacobian matrix to be of the form $(m \times n) \times (m \times n)$.  By plugging in the values, we get the following results:\\
\begin{tabular}{c|c|c|c|c|c}
     & $S_y$ & K & $\cdots$ & E & I   \\
     \hline
     $S_y$ & $(1-\sum\limits_{K\neq S_y, E}\alpha_{S_y,K})\mathbbm{1}$ & $\alpha_{KS_y}\mathbbm{1}$ & $\cdots$ & $\alpha_{ES_y}\mathbbm{1}-p_{S_y}^* \beta_1 \textbf{A}$ & $\alpha_{IS_y}-p_S^*\beta_2\textbf{A}$\\
     \hline
     \vdots & & $\ddots$ & & & \\
     \hline
     U & $\alpha_{SyU}\mathbbm{1}$ & $\alpha_{KU}\mathbbm{1}$ & $\cdots$ & $\alpha_{EU}\mathbbm{1}$ & $\alpha_{IU}\mathbbm{1}$\\
     \hline
     \vdots & & & $\ddots$ & & \\
     \hline
     E & $\alpha_{S_yE}\mathbbm{1}$ & $\alpha_{KE}\mathbbm{1}$ & $\cdots$ & $\alpha_{EE}\mathbbm{1}+p_S^*\beta_1\mathbbm{A}$ & $\alpha_{IE}\mathbbm{1}+p_S^*\beta_2\mathbbm{A}$\\
     \hline
     I & $\alpha_{S_yI}\mathbbm{1}$ & $\alpha_{KI}\mathbbm{1}$ & $\cdots$ & $\alpha_{EI}\mathbbm{1}$ & $\alpha_{II}\mathbbm{1}$
\end{tabular}\\
 Here, the state $K$ denotes all states not equal to $E$ or $I$, whereas $U \neq \left\lbrace S_1,...,S_w,E,I \right\rbrace$.  We shall now recall one of our previous assumptions, namely that there can be no endogenous transitions into the Infected class.  Mathematically speaking this means
 \begin{eqnarray}
 \forall K \neq E,I : \alpha_{KE} = \alpha_{KI} = 0
 \end{eqnarray}
 This, of course simplifies our Jacobian, which is reduced to \\
\begin{tabular}{c|c|c|c|c|c}
     & $S_y$ & K & $\cdots$ & E & I   \\
     \hline
     $S_y$ & $(1-\sum\limits_{K\neq S_y, E}\alpha_{S_y,K})\mathbbm{1}$ & $\alpha_{KS_y}\mathbbm{1}$ & $\cdots$ & $\alpha_{ES_y}\mathbbm{1}-p_{S_y}^* \beta_1 \textbf{A}$ & $\alpha_{IS_y}-p_S^*\beta_2\textbf{A}$\\
     \hline
     \vdots & & $\ddots$ & & & \\
     \hline
     U & $\alpha_{SyU}\mathbbm{1}$ & $\alpha_{KU}\mathbbm{1}$ & $\cdots$ & $\alpha_{EU}\mathbbm{1}$ & $\alpha_{IU}\mathbbm{1}$\\
     \hline
     \vdots & & & $\ddots$ & & \\
     \hline
     E & $\textbf{0}_{N\times N}$ & $\textbf{0}_{N\times N}$ & $\cdots$ & $\alpha_{EE}\mathbbm{1}+p_S^*\beta_1\mathbbm{A}$ & $\alpha_{IE}\mathbbm{1}+p_S^*\beta_2\mathbbm{A}$\\
     \hline
     I & $\textbf{0}_{N\times N}$ & $\textbf{0}_{N\times N}$ & $\cdots$ & $\alpha_{EI}\mathbbm{1}$ & $\alpha_{II}\mathbbm{1}$
\end{tabular}\\
 Now that we have attained our matrix, we must calculate all of its eigenvalues.
 \subsubsection{Calculating the Eigenvalues of our Jacobian}
As we can see in $(19)$, the Jacobian is of the form
 \begin{eqnarray}
  \mathcal{J}=
  \left( {\begin{array}{cc}
  \bold{B}_1 & \bold{B}_2 \\
   \bold{0} & \bold{B}_1 \\
  \end{array} } \right)
 \end{eqnarray}
 where $\bold{B}_1$ is a $((m-2) \cdot n) \times ((m-2)\cdot n)$ matrix, $\bold{B}_2$ is a $((m-2)\cdot n) \times (2\cdot n)$ matrix, and $\bold{B}_3$ is a $(2 \cdot n) \times (2\cdot n)$ matrix.  Any eigenvector $\bold{v}$ with eigenvalue $\lambda_ \mathcal{J}$ can now be thought of a composition of two smaller vectors, $\bold{v}_1$ and $\bold{v}_2$, such that 
  \begin{eqnarray}
  \bold{v} = \left(  \bold{v}_1,  \bold{v}_2 \right)^T .
  \end{eqnarray}
Since   $\bold{v}$ is an eigenvector, we know that
 \begin{eqnarray}
  \left( {\begin{array}{cc}
  \bold{B}_1 & \bold{B}_2 \\
   \bold{0} & \bold{B}_1 \\
  \end{array} } \right)
  \left( {\begin{array}{c} 
    \bold{v}_1 \\
 \bold{v}_2 \\
  \end{array} } \right) = \lambda_ \mathcal{J} \left( {\begin{array}{c} 
    \bold{v}_1 \\
 \bold{v}_2 \\
  \end{array} } \right) .
 \end{eqnarray}
 This, in turn, can be written as 
   \begin{eqnarray}
   \bold{B}_1   \bold{v}_1 +    \bold{B}_2   \bold{v}_2 &=& \lambda_ \mathcal{J}  \bold{v}_1 \\
\bold{B}_3   \bold{v}_2 &=& \lambda_ \mathcal{J}  \bold{v}_2 ,
  \end{eqnarray}
  which tells us that one of the following must be true: \\
  $(i)$ $ \bold{v}_2=0$ \\
  $(ii)$ $ \bold{v}_2$ can be thought of as an eigenvector of $\bold{B}_3$ with eigenvalue $\lambda_ \mathcal{J}$ \\
We shall now examine both cases.
\subsubsection{Case (i)}
  Scenario $(i)$ simplifies equation $(23)$ considerably, as the second term is dropped and we left with 
\begin{eqnarray}
 \bold{B}_1   \bold{v}_1 = \lambda_ \mathcal{J}  \bold{v}_1 ,
\end{eqnarray}
which then again again implies one of the following: \\
  $(i)$ $ \bold{v}_1=0$ \\
  $(ii)$ $ \bold{v}_1$ is an eigenvector \\
  Since $\bold{v}_2=0$ by assumption, $ \bold{v}_1=0$ would yield a zero vector as the solution to $(22)$, which is not a very interesting outcome (as $ \bold{v}=0 $ would not be considered an eigenvector).  We can therefore consider  $ \bold{v}_1$ to be an eigenvector of $\bold{B}_1$.  \\
  \underline{Definition:}
  For two arbitrary matrices $A$ and $B$, where $A$ and $B$ are of the form $m \times n$ and $r \times s$, respectively, the Kronecker product $A \otimes B$ is defined as
  \begin{eqnarray}
  A \otimes B =  \left( {\begin{array}{ccc}
 a_{11} \cdot B & ... &  a_{1n} \cdot B \\
  ... &... &... \\
  a_{m1} \cdot B & ... &  a_{mn} \cdot B 
  \end{array} } \right) .
  \end{eqnarray}
  This is clearly a $ (m \cdot r)  \times (n \cdot s)$ matrix. \\
  We shall now note that with the help of the Kronecker product, $\bold{B}_1$ can be written as
  \begin{eqnarray}
  \bold{B}_1   = \bold{T}   \otimes \mathbbm{1} ,
  \end{eqnarray}
  where $\bold{T} $ is given by
   \begin{eqnarray}
  \bold{T}   =  \left( {\begin{array}{ccc}
 \left( 1 - \sum_{K \neq S_y,E} \right)  & \alpha_{KS_y} &  \cdots\\
  \vdots &\vdots &\vdots \\
  \alpha_{S_yU} &  \alpha_{KU} & \cdots \\
    \vdots & \ddots &\vdots 
  \end{array} } \right) .
  \end{eqnarray}
  It can be shown that if $A=B \otimes C$, $A_{diag} = B_{diag} \otimes C_{diag}$. This means that if we diagonalize $A$, we have diagonalized $B$ and $C$ as well.  Now, since in our case $C$ is the identity, $\bold{T}$ will have the same eigenvalues as $\bold{B}_1$.  We shall now analyze case $(ii)$.
  \subsubsection{Case (ii)}
  Here, we know that $\bold{v}_2$ is an eigenvector of $\bold{B}_3$.  From $(19)$ it follows that 
  \begin{eqnarray}
\bold{B}_3 =   \left( {\begin{array}{cc}
  \alpha_{EE} \mathbbm{1} + p_S^{\star} \beta_1 \bold{A} &\alpha_{IE} \mathbbm{1} + p_S^{\star} \beta_2 \bold{A}\\
    \alpha_{EI} \mathbbm{1} &  \alpha_{II} \mathbbm{1}\\
  \end{array} } \right) .
  \end{eqnarray}
  For pragmatic reasons, we shall split $\bold{v}_2$ into to components, $\bold{u}_1$ and $\bold{u}_2$, such that both are vectors of length $n$.  Since $\bold{v}_2$ is an eigenvector with eigenvalue $\lambda_ \mathcal{J}$, we know that the two components satisfy:
  \begin{eqnarray}
  \left(  \alpha_{EE} \mathbbm{1} + p_S^{\star} \beta_1 \bold{A} \right) \bold{u}_1 + \left( \alpha_{IE} \mathbbm{1} + p_S^{\star} \beta_2 \bold{A} \right)  \bold{u}_2 &=& \lambda_ \mathcal{J} \bold{u}_1 \\ 
   \alpha_{E1} \bold{u}_1 +    \alpha_{II} \bold{u}_2 &=& \lambda_ \mathcal{J} \bold{u}_2 
  \end{eqnarray}
  Equation $(31)$ tells us that 
  \begin{equation}
  \bold{u}_1 = \left( \frac{\lambda_ \mathcal{J} -\alpha_{II}}{\alpha_{EI}} \right)  \bold{u}_2 ,
  \end{equation}
  which can be plugged into $(30)$ to get:
  \begin{eqnarray}
  \left( \left( \alpha_{EE} \mathbbm{1} + p_S^{\star} \beta_1 \bold{A} \right) \left( \frac{\lambda_ \mathcal{J} -\alpha_{II}}{\alpha_{EI}} \right)  + \left( \alpha_{IE} \mathbbm{1} + p_S^{\star} \beta_2 \bold{A} \right) \right)  \bold{u}_2 = \lambda_ \mathcal{J} \left( \frac{\lambda_ \mathcal{J} -\alpha_{II}}{\alpha_{EI}} \right)  \bold{u}_2 
  \end{eqnarray} 
  We can now reorder the equation such that we end up with:
  \begin{eqnarray}
  \bold{A} \bold{u}_2 = \left( \frac{\lambda_ \mathcal{J}^2 - \left(\alpha_{II} + \alpha_{EE} \right)  \lambda_ \mathcal{J} + \alpha_{II}  \alpha_{EE} - \alpha_{IE}  \alpha_{E1}}{p_S^{\star} \beta_1 \left(\lambda_ \mathcal{J} -\alpha_{II} \right) + p_S^{\star} \beta_2 \alpha_{EI}  } \right)  \bold{u}_2 
  \end{eqnarray}
  Since we know that $ \bold{u}_2  \neq 0$ (otherwise $ \bold{u}_1 =0$ and $ \bold{v} = 0$),  $ \bold{u}_2 $ is an eigenvector of the adjacency matrix. As we can see in $(34)$, the eigenvalue is dependent on many parameters.  To simplify matters, we shall call the eigenvalue $\lambda_{\bold{A}}$.  We can now replace $\bold{A}$ with its eigenvalue in equation $(33)$, so that it can be rewritten as:
  \begin{eqnarray}
  \lambda_ \mathcal{J}^2  -\lambda_ \mathcal{J} \left(\alpha_{II} + \alpha_{EE} +p_S^{\star} \beta_1 \lambda_{\bold{A}} \right) + \left( \alpha_{II}  \alpha_{EE} - \alpha_{IE}  \alpha_{E1}
+  p_S^{\star} \lambda_{\bold{A}} \left( \beta_1 \alpha_{II} - \beta_2 \alpha_{EI} \right) \right) =0
\end{eqnarray}
We've now found a quadratic equation that will give us two (possibly different) eigenvalues for  $\mathcal{J}$ as a function of eigenvalues of $\bold{A}$.  The calculations done in this chapter thusly show that the following lemma holds: \\
\underline{Lemma:} \\
The eigenvalues of $\mathcal{J}$ are either acquired via the matrix $\bold{T}$ or by solving the quadratic equation $(35)$, whose roots are dependent in the eigenvalues of $\bold{A}$.
\subsubsection{Stability Requirements}
The asymptotic stability theorem requires that all eigenvalues of $\mathcal{J}$ have an absolute value less than $1$.  We can now use the previous subsections to find out when this is true.  The previous lemma states that one of the following two conditions must hold: \\
$(i)$ The eigenvalues of $\bold{T}$ must all have an absolute value less then $1$. \\
$(ii)$ The solutions of the quadratic equation given by $(35)$ must all have an absolute value less than $1$(for all eigenvalues of $\bold{A}$). \\
Let's first analyze scenario $(1)$.
\subsubsection{Case $(i)$}
Now this is quite simple.  Since $\bold{T}$ is a stochastic matrix ( This means that summing over the values in each column will always return the value $1$), it can only have eigenvalues whose absolute value are less than $1$.  Our requirements are therefore automatically fulfilled.  It should be noted that $\bold{T}$ does not depend on $\bold{A}$.
\subsubsection{Case $(ii)$}
In this case, we must find the conditions for which our eigenvalues have an absolute value less than $1$.  To do this, we will prove the following lemma: \\
 \underline{Lemma:}  \\
 The solutions of the quadratic equations, which are given by $(35)$ for all eigenvalues of $\bold{A}$, have an absolute value less than $1$ if the following inequality holds:
 \begin{eqnarray}
 \lambda_1 p_S^{\star} \left( \frac{c \left( 1- \alpha_{II}\right) + \beta_2 \alpha_{EI}}{\left( 1- \alpha_{II}\right) \left( 1- \alpha_{EE}\right) - \alpha_{IE}  \alpha_{E1}} \right),
 \end{eqnarray}
where $\lambda_1$ is the largest eigenvalue of $\bold{A}$. \\
 \underline{Proof:}  \\
 We define $r_1$ and $r_2$ to be the solutions of equation $(35)$ for a given $\lambda_{\bold{A}}$.  We obviously want that
 \begin{eqnarray}
 |r_1| < 1 \  \& \ |r_1| < 1 .
 \end{eqnarray}
 While both $r_1$ and $r_2$ can in general be complex numbers, let us first analyze the solutions, where both are real numbers.  In this case we know that $r_1 , r_2 \in \left( -1,1 \right)$.
For a general quadratic equation $f(x)= ax^2 +bx+c$, where $a >0$, both solutions can lie in the desired interval if and only if the following conditions hold:
\begin{eqnarray}
a-c &>& 0 \\
a-b+c &>& 0 \\
a-b+c &>& 0
\end{eqnarray}
But for our quadratic equation, this means that:
\begin{eqnarray}
\alpha_{II} \alpha_{EE} - \alpha_{IE} \alpha_{EI} + p_S^{\star} \lambda_{\bold{A}} \left( \beta_1 \alpha_{II} -\beta_2 \alpha_{EI} \right) &<& 1 \\
1+\alpha_{EE} + \alpha_{II}  + p_S^{\star} \beta_1 + \alpha_{II} \alpha_{EE} - \alpha_{IE} \alpha_{EI} + p_S^{\star} \lambda_{\bold{A}} \left( \beta_1 \alpha_{II} -\beta_2 \alpha_{EI} \right) &>& 0 \\
1-\alpha_{EE} - \alpha_{II}  - p_S^{\star} \beta_1 + \alpha_{II} \alpha_{EE} - \alpha_{IE} \alpha_{EI} + p_S^{\star} \lambda_{\bold{A}} \left( \beta_1 \alpha_{II} -\beta_2 \alpha_{EI} \right) &>& 0 
\end{eqnarray}
We can now rewrite the last two equations to get
\begin{eqnarray}
 \lambda_{\bold{A}}  p_S^{\star} \left( \frac{-\beta_1 \left( 1+\alpha_{II} \right) + \beta_1 \alpha_{EI} }{\left( 1+ \alpha_{II}\right) \left( 1+ \alpha_{EE}\right)-\alpha_{IE}  \alpha_{E1}} \right) &<& 1 \\
  \lambda_{\bold{A}}  p_S^{\star} \left( \frac{\beta_1 \left( 1-\alpha_{II} \right) + \beta_1 \alpha_{EI} }{\left( 1- \alpha_{II}\right) \left( 1- \alpha_{EE}\right)-\alpha_{IE}  \alpha_{E1}} \right) &<& 1 .
\end{eqnarray}
This should hold for all eigenvalues of $\bold{A}$.  Since $\bold{A}$ is a symmetric real-valued matrix, it must be diagonalizable with real eigenvalues.  Additionally, we can use the Perron-Frobenius theorem, which states that the algebraically largest eigenvalue $\lambda_1$ must not only be a positive number, but must also have the largest magnitude(in comparison to all the other eigenvalues).This tells us that if equations $(44)$ and $(45)$ are valid for $ \lambda_{\bold{A}}=\lambda_1$, they must be valid for all other eigenvalues as well.  We also know that the following two inequalities have to hold:
\begin{eqnarray}
\left( 1+ \alpha_{II}\right) \left( 1+ \alpha_{EE}\right)-\alpha_{IE}  \alpha_{E1} &>& 
\left( 1- \alpha_{II}\right) \left( 1- \alpha_{EE}\right)-\alpha_{IE}  \alpha_{E1} \\
\beta_1 \left( 1-\alpha_{II} \right) + \beta_1 \alpha_{EI} &>& -\beta_1 \left( 1+\alpha_{II} \right) + \beta_1 \alpha_{EI}
\end{eqnarray}
These follow from the fact that all the parameters must be positive and less than or equal to $1$.  With these two inequalities, it should be clear that if $(45)$ holds, $(44)$ must hold as well.  We can thus conclude, that if the roots of the quadratic equation are real
\begin{eqnarray}
 \lambda_{1}  p_S^{\star} \left( \frac{\beta_1 \left( 1-\alpha_{II} \right) + \beta_1 \alpha_{EI} }{\left( 1- \alpha_{II}\right) \left( 1- \alpha_{EE}\right)-\alpha_{IE}  \alpha_{E1}} \right) < 1 .
\end{eqnarray}
must hold to attain values between $(-1,1)$.  On the other hand, if we allow for complex roots, we must allow the discriminant to be negative.  Since our quadratic equation has real coefficients, $r_1$ and $r_2$ must be the complex conjugate of one another.  This just means that $|r_1|=|r_2|= \sqrt{r_1 r_2}$.  It is also easy to show that the product of both roots is equal to $\frac{c}{a}$, where the quadratic equation is given by $ax^2+bx+c=0$.  This means that since we require all roots to have an absolute value of less than $1$, $c/a<1$.  This just means that the following inequality must hold:
\begin{eqnarray}
\left( \alpha_{II}  \alpha_{EE} - \alpha_{IE}  \alpha_{E1}
+  p_S^{\star} \lambda_{\bold{A}} \left( \beta_1 \alpha_{II} - \beta_2 \alpha_{EI} \right) \right) <1
\end{eqnarray}
By rearranging the equation though, it turns out that this gives us equation $(44)$.  We already know though, that $(44)$ is implied by $(45)$.  Thus, if the condition given by $(45)$ is fulfilled, the roots must have an absolute value of less than $1$.
\subsubsection{Finalizing the Result}
We have finally shown that our fixed point is stable if
\begin{eqnarray}
 \lambda_{1}  p_S^{\star} \left( \frac{\beta_1 \left( 1-\alpha_{II} \right) + \beta_1 \alpha_{EI} }{\left( 1- \alpha_{II}\right) \left( 1- \alpha_{EE}\right)-\alpha_{IE}  \alpha_{E1}} \right) < 1 .
\end{eqnarray}
By setting 
\begin{eqnarray}
C_{VPM}=p_S^{\star} \left( \frac{\beta_1 \left( 1-\alpha_{II} \right) + \beta_1 \alpha_{EI} }{\left( 1- \alpha_{II}\right) \left( 1- \alpha_{EE}\right)-\alpha_{IE}  \alpha_{E1}} \right),
\end{eqnarray}
we have finally proven the G2 stability theorem.
\section{Experimental Verification of the Model}
Our aim is to reproduce the model as well as to push it towards its boundaries and find cases, where the epidemic threshold is not described by the effective strength. To do this, we conduct SIR simulations by implementing the Gillespie algorithm, which is described in the following section. Initially, we focus on a setting similar to the one used by \cite{original_paper}. In a further step, we vary the simulation parameters to find possible weaknesses of the model.  In particular, we focus on the influence of the initially infected nodes.
\subsection{Simulating the Time Evolution of the SIR model on Arbitrary Networks with the Gillespie Algorithm}
During the course of this chapter, we will analyze the time evolution of the propagating virus.  To do this, we consider the fraction of infected people and the threshold condition for an outbreak of an epidemic in the SIR model shown in Figure 5. \\
The SIR model states that individuals can either be susceptible for the disease (S), infected (I) or recovered (R). The only two possible events are the following: 
\begin{enumerate}
\item[(a)] An infected node infects a susceptible neighbour 
\item[(b)] An infected node gets recovered
\end{enumerate}
Hereinafter we will call the set of susceptible nodes that have at least one infected neighbor "at\_risk\_nodes". 
The model is entirely described by the transmission probability over a contact-link $\beta$ and the healing probability once infected $\delta$. The parameter $\beta$ is defined such that the probability that (a) happens in the infinitesimal time interval $[t, t+dt)$ is given by $\beta dt$. Analogously the probability that an infected node at time $t$ gets healed (event (b)) within the time interval $[t,t+dt)$ is $\delta dt$. Therefore the probability that exactly one event happens in the time interval $[t,t+dt)$ is given by 
\begin{align*}
\text{total\_rate}\times dt &= (\text{total\_infection\_rate}+\text{total\_recovery\_rate}(t))\times dt\\
\text{total\_infection\_rate} &= \sum\limits_{u\in \text{at\_risk\_nodes}}\beta\times\text{number\_of\_infected\_neighbours}[u]\\
\text{total\_recovery\_rate}(t)&=\delta\times\text{number\_of\_infected\_nodes}
\end{align*}
To solve this problem numerically we have to discretize the time and therefore replace $dt$ with a finite time step $\Delta t$. The smaller $\Delta t$ is chosen, the more accurate the algorithm describes the model. \\
We first want to describe a naive algorithm that could simulate the disease outbreak with the SIR model. Assuming we know the state of all nodes at time $t$, we can simulate the states of the next time step $t+\Delta t$ with the following algorithm: 
\begin{enumerate}
\item generate a random number $r_1$ uniformly distributed in the interval $(0,1)$
\item  \begin{itemize}
\item[(a)] If $r_1<\text{total\_rate}\times \Delta t$: generate another number $r_2$ uniformly distributed in the interval $(0, \text{total\_rate})$
\begin{itemize}
\item If $\text{total\_recovery\_rate} < r_2$: choose a random infected node $u$, set its state to "recovered", and reduce the number\_of\_infected\_neighbors[$\nu$] for every neighbor $\nu$ of $u$ by 1
\item Else: Choose $u$ from at\_risk\_nodes with probability \\
$\frac{\beta\times\text{number\_of\_infected\_neighbours}[u]}{\text{total\_infection\_rate}}$, remove it from the at\_risk\_nodes, set its state to infected and add its neighbors to at\_risk\_nodes
\item Update all rates and start over again with 1. for the time $t+\Delta t$
\end{itemize}
\item[(b)] Else: No event occurs in this time interval and we start again with 1. for the time $t+\Delta t$
\end{itemize}
\end{enumerate} 
Since $r_1$ is a random number uniformly distributed in $(0,1)$, the probability that $r_2<\text{total\_recovery\_rate}\times\Delta t$ is equal to $\text{total\_recovery\_rate}\times\Delta t$. So for infinitely small $\Delta t$, 2 (a) would exactly simulate the probability that an event will happen in the time step $t+\Delta t$. If an event happens in the considered time interval, the next  question is which event that is. The probability that a node recovers during this time-step is given by the probability that $\text{total\_recovery\_rate} <r_2$. In the remaining cases this event will be a transmission of the disease. The probability that a at\_risk\_node gets infected is simply proportional to its number of infected neighbors. \\
The problem with this algorithm is that by decreasing $\Delta t$ (and therefor increasing the accuracy), the computational intensity increases. During most of the time steps, the generated random number $r_1$ is created, even though no event took place.\\
The Gillespie follows a different approach: Instead of analyzing each time step, it calculates the time $t+\tau$ at which the next event occurs. It is a model that was developed in the middle of the $20^{th}$ century and was mainly used to efficiently simulate chemical or biochemical reactions \cite{wiki:Gillespie}. Instead of explaining the general algorithm, we want to focus on how it can be used to simulate the SIR model. Our paper will closely follow the work done in \cite{paper_gillespie}, where it's shown how the Gillespie algorithm can be used to describe chemical reactions.
\\
We shall define $f(\text{total\_rate}(t), s)ds$ as the probability that for a given $\text{total\_rate}$ at time $t$, the next event will happen during the time interval $[t+s,d+s+ds)$ and $g(\text{total\_rate}(t),s)ds$ as the probability, that no event happens in the time interval $[t,t+s)$. \\
The probability that for a given time $t$ the next event will happen during the time interval $[t+s,d+s+ds)$ can now be written as the product of the probabilities, that no event happens in the time interval $[t,t+s)$ and the probability that exactly one event happens in the time interval $[t+s,t+s+ds)$: $f(\text{total\_rate}(t), s)ds = g(\text{total\_rate}(t), s)\times\text{total\_rate}(t+s)ds$. Since no event happens in $[t,t+s)$ we know that $\text{total\_rate}(t+s)=\text{total\_rate}(t)$. Therefore: 
\begin{align}
f(\text{total\_rate}(t), s)ds = g(\text{total\_rate}(t), s)\times\text{total\_rate}(t)ds \label{f}
\end{align}
Now we want to calculate $g(\text{total\_rate}(t), \sigma )$ for all $\sigma>0$. For an infinitesimal time step $d\sigma$ the probability that no event occurs in the time interval $[t,t+\sigma+d\sigma)$  is given by: $g(\text{total\_rate}(t), \sigma+d\sigma)=g(\text{total\_rate}(t), \sigma)[1-\text{total\_rate}(t+\sigma)]d\sigma$. Here we have used the fact that $d\sigma$ is infinitesimally small and therefore the probability that more than one event happens in the time interval $[t+\sigma, t+\sigma+d\sigma)$ is negligible. In this expression we can replace $\text{total\_rate}(t+\sigma)$ with $\text{total\_rate}(t)$, since no event took place in the the time interval $[t,t+\sigma)$. This leads to the following differential equation for $g(\text{total\_rate}(t), \sigma)$: \begin{align*}
\frac{dg(\text{total\_rate}(t), \sigma)}{d\sigma} &= \lim\limits_{d\sigma\to0}\frac{g(\text{total\_rate}(t),\sigma+d\sigma)-g(\text{total\_rate}(t),\sigma)}{d\sigma}\\
&=-\text{total\_rate}(t)\times g(\text{total\_rate}(t),\sigma)
\end{align*}
With the initial condition that $g(\text{total\_rate}(t), 0)=1$, we obtain the unique solution $g(\text{total\_rate}(t), \sigma)=\exp[-\text{total\_rate}(t)\times\sigma]$. From this we know that $f(\text{total\_rate}(t), s)ds\overset{(\ref{f})} = \exp[-\text{total\_rate}(t)s]\times\text{total\_rate}(t)ds$. Therefore the time $\tau$, such that the next event happens at the time $t+\tau$, is exponentially distributed according to the probability distribution $f(\lambda, x)=\lambda\exp[-\lambda x]$ where $\lambda = \text{total\_rate}(t)$. \\
A Pseudocode for the Gillespie algorithm, which can be used to with the SIR model, is shown in Figure \ref{figure: Gillespie-pseudo}.\\
\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{figures/algorithm}
\caption{Pseudocode for the Gillespie algorithm simulating an SIR epidemic in a network \cite{Math_on_ep}}
\label{figure: Gillespie-pseudo}
\end{figure}
Assuming all states of the system are known for a time $t_0$, the algorithm perform the following steps:  
\begin{enumerate}
\item Compute all rates
\item Compute the time $t+\tau$ at which the next event will happen. Therefore the random number $\tau$ exponentially distributed according to $f(s) = \exp[-\text{total\_rate}(t)s]\times\text{total\_rate}(t)$ is generated
\item  Compute which event will happen
\begin{itemize}
\item[(a)] If $\text{total\_recovery\_rate} < r_2$: choose a random infected node $u$, set its state to "recovered" and reduce the number\_of\_infected\_neighbors[$\nu$] for every neighbor $\nu$ of $u$ by 1
\item[(b)] Else: Choose $u$ from at\_risk\_nodes with probability \\
$\frac{\beta\times\text{number\_of\_infected\_neighbours}[u]}{\text{total\_infection\_rate}}$, remove it from the at\_risk\_nodes, set its state to infected and add its neighbors to at\_risk\_nodes
\end{itemize}
\item Update all rates and start over again with 1. for the time $t+\tau$
\end{enumerate} 
\subsection{Centrality Measures for Initial Infections}
\label{centrality}
In order to initially infect the most important nodes we use the mathematical concept of centrality. There exist several different centrality measures, however, for our purposes the concept of eigenvector centrality turned out to be suitable.
The vector of the eigenvector centralities x of all nodes in a network is defined via
\begin{equation}
\bold{A}x=\lambda x
\end{equation}
where A denotes the networks’ adjacency matrix and lambda denotes the largest eigenvalue of A, which is real and positive by the Perron-Frobenius theorem. Hence, x is the eigenvector of the spectral radius of A. \cite{wiki:centrality}
\subsection{Implementation}
For our simulations we used an implementation of the Gillespie algorithm from \cite{Math_on_ep} as well as Python functions enabling us to plot the time behaviour of the infection and the footprint of the infection depending on the effective strength. Our Code uses the libraries scipy, matplotlib, networkX and EoN.
\subsubsection*{Data sets}
\begin{enumerate}
\item[i] AS-OREGON: Internet connection data collected by University of Oregon Route Views Project
\item[ii] FACEBOOK-EGO: Data from a survey on facebook app usage
\item[iii] TERRORISTS:
\end{enumerate}
\subsection{Simulation Results and Discussion}
This section closely follows the full test provided in the code section of the GIT repository belonging to this paper. All the calculations were conducted on an ordinary personal computer using unmodified Python 3.7. For the plots we ran at least 200 independent simulations each and plotted their arithmetic mean.
\subsubsection*{Reproduction of previous work}
To reproduce the work in \cite{original_paper} we used the AS-OREGON data set, which is similar to one of the data sets in their experiments. Initially we infected 10 random nodes and investigated the time behaviour of the SIR simulation, which is shown in figure \textbf{(1a)}. The result matches the expectations, as s-Values greater 1 lead to a significant increase of infected nodes while under-critical infections ($s < 1$) die out quickly. This behaviour is confirmed by figure \textbf{(2)}, which supports the assumption, the tipping point is around 1.
\subsubsection*{Initial infection size}
As we were able to reproduce the results from \cite{original_paper}, we investigated the influence of the size of the initially infected population. So, we ran the same simulation as beforehand with different numbers of randomly selected nodes infected in the beginning. The result is shown in figure \textbf{(3a)}, which suggests, that the model is still valid, even for a notable size of initially infected nodes. However, the tipping point slightly moved to the left with increasing number of initial infections.
\subsubsection*{Infection of important nodes}
In the next step, we initially infected the nodes with the highest eigenvector centrality (see section \ref{centrality}). As expected, this is beneficial to the spreading of the virus. Figure \textbf{(4)} compares the footprint of random infection with critical node infection and depicts, that the initial infection of the nodes with the highest eigenvalue centrality has a tipping point moved to the left, even slightly beyond 1. Hence, infection of critical nodes brings the model already close to its border.

Additionally, we conducted experiments with larger numbers of critical initial infection. The results, shown in figure \textbf{(3b)}, indicate again that initial infection of the nodes with the highest centrality leads to a tipping point very close to 1, particularly for larger numbers of initially infected nodes. However, the model can still be considered valid, as the footprint for effective strengths below 1 remains very close to the initial infection. This meets, due to the generality of the proof (see section \ref{proof}), our expectations.
\subsubsection*{Verification with highly connected network}
To verify the results of the previous section, we ran the very same simulation on a different network. We used the FACEBOOK-EGO network, which has a slightly smaller number of nodes, but nevertheless a larger spectral radius and therefore a higher density of connections, and were able to reproduce all the outcomes from the AS-OREGON network. As expected, the model is also valid for all our test cases on this network.
\subsubsection*{Behaviour of small networks}
As the behavior for very large graphs was already investigated in \cite{original_paper} using the PORTLAND graph with 1.6 million nodes, we were interested if the model also applicable for small networks. Hence, we ran simulations on the TERRORISTS network with 62 nodes and a high density of connections. We again infected the nodes with the highest eigenvector centrality. Figure \textbf{(5)} shows the results for different sizes of the initial infection. Now the tipping point is, especially for larger numbers of initially infected nodes, below one. This is interesting, as for the larger networks even higher rates of initially infected nodes led to no disease spreading. Hence, we draw the conclusion that the model is not generally valid for small networks.
\section{Summary and Outlook}


\bibliographystyle{plain}
\bibliography{bibfile}






\end{document}  



 
